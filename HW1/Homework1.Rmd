---
title: 'STA 5635: Homework 1'
date: 'Due: Friday, January 16th by 11:59 PM'
name: Bryce St.Pierre
output:
  word_document: default
  html_document:
    df_print: paged
  pdf_document: default
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Use a programming language or package where decision trees and random
forests can be trained and applied. Examples include Matlab, Python
(scikit-learn package), or R.

```{r results = 'hide'}
# Load in packages
library(tidymodels)
library(tidyr)
```

```{r}
# Load in the SAT datasets
SATXTest <- read.csv("C:/Users/Bryce/Downloads/STA 5635/HW1/satimage/Xtest.dat", header = FALSE, sep = " ")

SATX <- read.csv("C:/Users/Bryce/Downloads/STA 5635/HW1/satimage/X.dat", header = FALSE, sep = " ")

SATYTest <- read.csv("C:/Users/Bryce/Downloads/STA 5635/HW1/satimage/Ytest.dat", header = FALSE, sep = " ")

SATY <- read.csv("C:/Users/Bryce/Downloads/STA 5635/HW1/satimage/Y.dat", header = FALSE, sep = " ")

SATTrain <- bind_cols(as_tibble(SATX), class = as.factor(SATY$V1))

SATTest <- bind_cols(as_tibble(SATXTest), class = as.factor(SATYTest$V1))
```

```{r}
# Load in the MAD datasets
MADTest <- read.csv("C:/Users/Bryce/Downloads/STA 5635/HW1/MADELON/madelon_test.data", header = FALSE, sep = " ")

MADTrain <- read.csv("C:/Users/Bryce/Downloads/STA 5635/HW1/MADELON/madelon_train.data", header = FALSE, sep = " ")

MADTrainLabels <- read.csv("C:/Users/Bryce/Downloads/STA 5635/HW1/MADELON/madelon_train.labels", header = FALSE, sep = " ")

MADValid <- read.csv("C:/Users/Bryce/Downloads/STA 5635/HW1/MADELON/madelon_valid.data", header = FALSE, sep = " ")

MADValidLabels <- read.csv("C:/Users/Bryce/Downloads/STA 5635/HW1/MADELON/madelon_valid.labels", header = FALSE, sep = " ")

MADTrain <- bind_cols(as_tibble(MADTrain), class = as.factor(MADTrainLabels$V1)) %>% select (-c(V501))

MADValid <- bind_cols(as_tibble(MADValid), class = as.factor(MADValidLabels$V1)) %>% select (-c(V501))
```

Using the training and test sets specified in the syllabus, perform the
following tasks:

**a)** On the madelon dataset, train decision trees of maximum depth 1,
2, .... up to 12, for a total of 12 decision trees. If your package does
not allow the max depth as a parameter, train trees with 21, 22, ...,
212 nodes, again a total of 12 trees. Use the trained trees to predict
the class labels on the training and test sets, and obtain the training
and test misclassification errors. Plot on the same graph the training
and test misclassification errors vs tree depth (or log2 of nodes) as
two separate curves. Report in a table the minimum test error and the
tree depth (number of nodes or splits) for which the minimum was
attained. (2 points)

```{r}

MADResults <- tibble(depth = integer(), train_error = double(), test_error = double())

for (depth in 1:12) {
  tree_spec <- decision_tree(tree_depth = depth) %>% 
    set_engine("rpart") %>% 
    set_mode("classification")
  
  tree_fit <- tree_spec %>% 
    fit(class ~ ., data = MADTrain)
  
  train_preds <- predict(tree_fit, MADTrain, type = "class")
  test_preds <- predict(tree_fit, MADValid, type = "class")
  
  train_error <- mean(train_preds$.pred_class != MADTrain$class)
  test_error <- mean(test_preds$.pred_class != MADValid$class)
  
  MADResults <- MADResults %>% 
    add_row(depth = depth, train_error = train_error, test_error = test_error)
}

```

```{r}
print(MADResults)
```

-   The minimum depth and validation error are given by 6 and 0.1966667
    respectively.

```{r}
ggplot(MADResults, aes(x = depth)) +
  geom_line(aes(y = train_error, color = "Training Error")) +
  geom_line(aes(y = test_error, color = "Testing Error")) +
  labs(title = "Misclassification Error vs Tree Depth",
       x = "Tree Depth",
       y = "Misclassification Error",
       color = "Error Type")
```

**b)** Repeat point a) on the satimage dataset. (2 points)

```{r}

SATResults <- tibble(depth = integer(), train_error = double(), test_error = double())

for (depth in 1:12) {
  tree_spec <- decision_tree(tree_depth = depth) %>% 
    set_engine("rpart") %>% 
    set_mode("classification")
  
  tree_fit <- tree_spec %>% 
    fit(class ~ ., data = SATTrain)
  
  train_preds <- predict(tree_fit, SATTrain, type = "class")
  test_preds <- predict(tree_fit, SATTest, type = "class")
  
  train_error <- mean(train_preds$.pred_class != SATTrain$class)
  test_error <- mean(test_preds$.pred_class != SATTest$class)
  
  SATResults <- SATResults %>% 
    add_row(depth = depth, train_error = train_error, test_error = test_error)
}

```

```{r}
print(SATResults)
```

-   The minimum depth and testing error are given by 5 and 0.2170
    respectively.

```{r}
ggplot(SATResults, aes(x = depth)) +
  geom_line(aes(y = train_error, color = "Training Error")) +
  geom_line(aes(y = test_error, color = "Testing Error")) +
  labs(title = "Misclassification Error vs Tree Depth",
       x = "Tree Depth",
       y = "Misclassification Error",
       color = "Error Type")
```

**c)** On the madelon dataset, for each of k ∈ {3, 10, 30, 100, 300}
train a random forest with k trees where the split attribute at each
node is chosen from a random subset of 22 ≈ √500 features. Use the
trained trees to predict the class labels on the training and test sets,
and obtain the training and test misclassification errors. Plot on the
same graph the training and test errors vs number of trees k as two
separate curves. Report the training and test misclassification errors
in a table. (2 points)

```{r}

MADResults2 <- tibble(k = integer(), train_error = double(), test_error = double())

for (k in c(3, 10, 30, 100, 300)) {
  rf_spec <- rand_forest(trees = k, mtry = 22) %>% 
    set_engine("ranger") %>% 
    set_mode("classification")
  
  rf_fit <- rf_spec %>% 
    fit(class ~ ., data = MADTrain)
  
  train_preds <- predict(rf_fit, MADTrain, type = "class")
  test_preds <- predict(rf_fit, MADValid, type = "class")
  
  train_error <- mean(train_preds$.pred_class != MADTrain$class)
  test_error <- mean(test_preds$.pred_class != MADValid$class)
  
  MADResults2 <- MADResults2 %>% 
    add_row(k = k, train_error = train_error, test_error = test_error)
}

```

```{r}
print(MADResults2)
```

```{r}
ggplot(MADResults2, aes(x = k)) +
  geom_line(aes(y = train_error, color = "Training Error"), size = 1.2) +
  geom_line(aes(y = test_error, color = "Testing Error"), size = 1.2) +
  scale_x_continuous(breaks = c(3, 10, 30, 100, 300)) +
  labs(title = "Misclassification Error vs Number of Trees",
       x = "Number of Trees (k)",
       y = "Misclassification Error",
       color = "Error Type")
```

**d)** Repeat point c) on the madelon dataset where the split attribute
at each node is chosen from a random subset of 9 ≈ log2(500) features.
(1 point)

```{r}
MADResults3 <- tibble(k = integer(), train_error = double(), test_error = double())

for (k in c(3, 10, 30, 100, 300)) {
  rf_spec <- rand_forest(trees = k, mtry = 22) %>% 
    set_engine("ranger") %>% 
    set_mode("classification")
  
  rf_fit <- rf_spec %>% 
    fit(class ~ ., data = MADTrain)
  
  train_preds <- predict(rf_fit, MADTrain, type = "class")
  test_preds <- predict(rf_fit, MADValid, type = "class")
  
  train_error <- mean(train_preds$.pred_class != MADTrain$class)
  test_error <- mean(test_preds$.pred_class != MADValid$class)
  
  MADResults3 <- MADResults3 %>% 
    add_row(k = k, train_error = train_error, test_error = test_error)
}
```

```{r}
print(MADResults3)
```

```{r}
ggplot(MADResults3, aes(x = k)) +
  geom_line(aes(y = train_error, color = "Training Error"), size = 1.2) +
  geom_line(aes(y = test_error, color = "Testing Error"), size = 1.2) +
  scale_x_continuous(breaks = c(3, 10, 30, 100, 300)) +
  labs(title = "Misclassification Error vs Number of Trees",
       x = "Number of Trees (k)",
       y = "Misclassification Error",
       color = "Error Type")
```

**e)** Repeat point c) on the madelon dataset where the split attribute
at each node is chosen from all 500 features. (1 point)

```{r}
MADResults4 <- tibble(k = integer(), train_error = double(), test_error = double())

for (k in c(3, 10, 30, 100, 300)) {
  rf_spec <- rand_forest(trees = k, mtry = 22) %>% 
    set_engine("ranger") %>% 
    set_mode("classification")
  
  rf_fit <- rf_spec %>% 
    fit(class ~ ., data = MADTrain)
  
  train_preds <- predict(rf_fit, MADTrain, type = "class")
  test_preds <- predict(rf_fit, MADValid, type = "class")
  
  train_error <- mean(train_preds$.pred_class != MADTrain$class)
  test_error <- mean(test_preds$.pred_class != MADValid$class)
  
  MADResults4 <- MADResults4 %>% 
    add_row(k = k, train_error = train_error, test_error = test_error)
}
```

```{r}
print(MADResults4)
```

```{r}
ggplot(MADResults4, aes(x = k)) +
  geom_line(aes(y = train_error, color = "Training Error"), size = 1.2) +
  geom_line(aes(y = test_error, color = "Testing Error"), size = 1.2) +
  scale_x_continuous(breaks = c(3, 10, 30, 100, 300)) +
  labs(title = "Misclassification Error vs Number of Trees",
       x = "Number of Trees (k)",
       y = "Misclassification Error",
       color = "Error Type")
```
